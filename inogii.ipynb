{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>is_weekend</th>\n",
       "      <th>steps_total</th>\n",
       "      <th>distance</th>\n",
       "      <th>sleep</th>\n",
       "      <th>app_usage</th>\n",
       "      <th>home_cluster</th>\n",
       "      <th>clusters_count</th>\n",
       "      <th>t_sport</th>\n",
       "      <th>practiced_sport</th>\n",
       "      <th>valence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>2019-05-31</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.161625</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>2019-06-01</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.992985</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>2019-06-02</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.038469</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>2019-06-03</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.199408</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>2019-06-04</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.185056</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>2019-06-05</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.481083</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>5</td>\n",
       "      <td>2019-06-06</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.854461</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.157992</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>5</td>\n",
       "      <td>2019-06-07</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.416468</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.216220</td>\n",
       "      <td>2.249959</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.696035</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>5</td>\n",
       "      <td>2019-06-08</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.153967</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.958696</td>\n",
       "      <td>1.951986</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.729374</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>5</td>\n",
       "      <td>2019-06-09</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.930487</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.755388</td>\n",
       "      <td>0.561072</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>5</td>\n",
       "      <td>2019-06-10</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.802933</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.726067</td>\n",
       "      <td>1.367852</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>5</td>\n",
       "      <td>2019-06-11</td>\n",
       "      <td>0</td>\n",
       "      <td>1.213865</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.068913</td>\n",
       "      <td>0.969952</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.477886</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>5</td>\n",
       "      <td>2019-06-12</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.635887</td>\n",
       "      <td>-0.378146</td>\n",
       "      <td>-0.113657</td>\n",
       "      <td>0.906299</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.598946</td>\n",
       "      <td>-0.788046</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>5</td>\n",
       "      <td>2019-06-13</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.693118</td>\n",
       "      <td>-0.370868</td>\n",
       "      <td>0.845686</td>\n",
       "      <td>1.647480</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.419785</td>\n",
       "      <td>-0.682122</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>5</td>\n",
       "      <td>2019-06-14</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.719938</td>\n",
       "      <td>-0.365948</td>\n",
       "      <td>-0.120525</td>\n",
       "      <td>1.394814</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.061462</td>\n",
       "      <td>-0.795396</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>5</td>\n",
       "      <td>2019-06-15</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.776112</td>\n",
       "      <td>-0.290275</td>\n",
       "      <td>1.132937</td>\n",
       "      <td>1.358679</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.330204</td>\n",
       "      <td>-0.709161</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>5</td>\n",
       "      <td>2019-06-16</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.102439</td>\n",
       "      <td>-0.267615</td>\n",
       "      <td>-0.079908</td>\n",
       "      <td>1.599115</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.285414</td>\n",
       "      <td>-0.625419</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>5</td>\n",
       "      <td>2019-06-17</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.793218</td>\n",
       "      <td>-0.375127</td>\n",
       "      <td>-0.238534</td>\n",
       "      <td>0.661693</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.330204</td>\n",
       "      <td>-0.435753</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>5</td>\n",
       "      <td>2019-06-18</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.686993</td>\n",
       "      <td>-0.368235</td>\n",
       "      <td>-1.362330</td>\n",
       "      <td>3.556651</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.240624</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>5</td>\n",
       "      <td>2019-06-19</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.698819</td>\n",
       "      <td>-0.371665</td>\n",
       "      <td>-1.111765</td>\n",
       "      <td>1.625938</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.778108</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    id        date  is_weekend  steps_total  distance     sleep  app_usage  \\\n",
       "0    5  2019-05-31           0          NaN       NaN       NaN  -0.161625   \n",
       "1    5  2019-06-01           1          NaN       NaN       NaN   1.992985   \n",
       "2    5  2019-06-02           1          NaN       NaN       NaN   1.038469   \n",
       "3    5  2019-06-03           0          NaN       NaN       NaN   1.199408   \n",
       "4    5  2019-06-04           0          NaN       NaN       NaN   2.185056   \n",
       "5    5  2019-06-05           0          NaN       NaN       NaN   2.481083   \n",
       "6    5  2019-06-06           0    -0.854461       NaN       NaN   1.157992   \n",
       "7    5  2019-06-07           0    -0.416468       NaN -1.216220   2.249959   \n",
       "8    5  2019-06-08           1    -0.153967       NaN -0.958696   1.951986   \n",
       "9    5  2019-06-09           1    -0.930487       NaN -0.755388   0.561072   \n",
       "10   5  2019-06-10           0    -0.802933       NaN -0.726067   1.367852   \n",
       "11   5  2019-06-11           0     1.213865       NaN  0.068913   0.969952   \n",
       "12   5  2019-06-12           0    -0.635887 -0.378146 -0.113657   0.906299   \n",
       "13   5  2019-06-13           0    -0.693118 -0.370868  0.845686   1.647480   \n",
       "14   5  2019-06-14           0    -0.719938 -0.365948 -0.120525   1.394814   \n",
       "15   5  2019-06-15           1    -0.776112 -0.290275  1.132937   1.358679   \n",
       "16   5  2019-06-16           1    -0.102439 -0.267615 -0.079908   1.599115   \n",
       "17   5  2019-06-17           0    -0.793218 -0.375127 -0.238534   0.661693   \n",
       "18   5  2019-06-18           0    -0.686993 -0.368235 -1.362330   3.556651   \n",
       "19   5  2019-06-19           0    -0.698819 -0.371665 -1.111765   1.625938   \n",
       "\n",
       "    home_cluster  clusters_count   t_sport  practiced_sport  valence  \n",
       "0            NaN             NaN       NaN              NaN      NaN  \n",
       "1            NaN             NaN       NaN              NaN      NaN  \n",
       "2            NaN             NaN       NaN              NaN      NaN  \n",
       "3            NaN             NaN       NaN              NaN      NaN  \n",
       "4            NaN             NaN       NaN              NaN      NaN  \n",
       "5            NaN             NaN       NaN              NaN      NaN  \n",
       "6            NaN             NaN       NaN              NaN      0.0  \n",
       "7            NaN             NaN -0.696035              0.0      1.0  \n",
       "8            NaN             NaN -0.729374              0.0      0.0  \n",
       "9            NaN             NaN       NaN              NaN      NaN  \n",
       "10           NaN             NaN       NaN              NaN      NaN  \n",
       "11           NaN             NaN -0.477886              1.0      NaN  \n",
       "12           NaN       -0.598946 -0.788046              0.0      NaN  \n",
       "13           NaN       -0.419785 -0.682122              0.0      NaN  \n",
       "14           NaN       -0.061462 -0.795396              0.0      NaN  \n",
       "15           NaN       -0.330204 -0.709161              0.0      NaN  \n",
       "16           NaN       -0.285414 -0.625419              1.0      NaN  \n",
       "17           NaN       -0.330204 -0.435753              1.0      NaN  \n",
       "18           NaN       -0.240624       NaN              NaN      NaN  \n",
       "19           NaN       -0.778108       NaN              NaN      NaN  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## sort by id and date columns\n",
    "df.sort_values(by=['id', 'date'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>is_weekend</th>\n",
       "      <th>steps_total</th>\n",
       "      <th>distance</th>\n",
       "      <th>sleep</th>\n",
       "      <th>app_usage</th>\n",
       "      <th>home_cluster</th>\n",
       "      <th>clusters_count</th>\n",
       "      <th>t_sport</th>\n",
       "      <th>practiced_sport</th>\n",
       "      <th>valence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>2019-05-31</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.161625</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>2019-06-01</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.992985</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>2019-06-02</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.038469</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>2019-06-03</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.199408</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>2019-06-04</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.185056</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110692</th>\n",
       "      <td>2689</td>\n",
       "      <td>2020-04-27</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.985184</td>\n",
       "      <td>-0.377994</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.796709</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110693</th>\n",
       "      <td>2689</td>\n",
       "      <td>2020-04-28</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.952450</td>\n",
       "      <td>-0.379999</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.796709</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110694</th>\n",
       "      <td>2689</td>\n",
       "      <td>2020-04-29</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110695</th>\n",
       "      <td>2689</td>\n",
       "      <td>2020-04-30</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.936189</td>\n",
       "      <td>-0.380235</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.796709</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110696</th>\n",
       "      <td>2689</td>\n",
       "      <td>2020-05-01</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.855095</td>\n",
       "      <td>-0.379554</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.796709</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>110697 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id        date  is_weekend  steps_total  distance  sleep  app_usage  \\\n",
       "0          5  2019-05-31           0          NaN       NaN    NaN  -0.161625   \n",
       "1          5  2019-06-01           1          NaN       NaN    NaN   1.992985   \n",
       "2          5  2019-06-02           1          NaN       NaN    NaN   1.038469   \n",
       "3          5  2019-06-03           0          NaN       NaN    NaN   1.199408   \n",
       "4          5  2019-06-04           0          NaN       NaN    NaN   2.185056   \n",
       "...      ...         ...         ...          ...       ...    ...        ...   \n",
       "110692  2689  2020-04-27           0    -0.985184 -0.377994    NaN        NaN   \n",
       "110693  2689  2020-04-28           0    -0.952450 -0.379999    NaN        NaN   \n",
       "110694  2689  2020-04-29           0          NaN       NaN    NaN        NaN   \n",
       "110695  2689  2020-04-30           0    -0.936189 -0.380235    NaN        NaN   \n",
       "110696  2689  2020-05-01           0    -0.855095 -0.379554    NaN        NaN   \n",
       "\n",
       "        home_cluster  clusters_count   t_sport  practiced_sport  valence  \n",
       "0                NaN             NaN       NaN              NaN      NaN  \n",
       "1                NaN             NaN       NaN              NaN      NaN  \n",
       "2                NaN             NaN       NaN              NaN      NaN  \n",
       "3                NaN             NaN       NaN              NaN      NaN  \n",
       "4                NaN             NaN       NaN              NaN      NaN  \n",
       "...              ...             ...       ...              ...      ...  \n",
       "110692           NaN             NaN -0.796709              0.0      0.0  \n",
       "110693           NaN             NaN -0.796709              0.0      1.0  \n",
       "110694           NaN             NaN       NaN              NaN      0.0  \n",
       "110695           NaN             NaN -0.796709              0.0      2.0  \n",
       "110696           NaN             NaN -0.796709              0.0      NaN  \n",
       "\n",
       "[110697 rows x 12 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_columns_mean(df):\n",
    "    columns_to_impute = ['steps_total', 'distance', 'sleep', 'app_usage', 'home_cluster', 't_sport', 'clusters_count']\n",
    "\n",
    "    for column in columns_to_impute:\n",
    "        # First, impute missing values with the mean within each group\n",
    "        df[column] = df.groupby('id')[column].transform(lambda x: x.fillna(x.mean()))\n",
    "        df[f'{column}_maxdiff'] = df.groupby('id')[column].transform(lambda x: x.max() - x)\n",
    "        df[f'{column}_mindiff'] = df.groupby('id')[column].transform(lambda x: x - x.min())\n",
    "        df[f'{column}_meandiff'] = df.groupby('id')[column].transform(lambda x: x - x.mean())\n",
    "        # Then, for any remaining NaN values, impute with the global column mean\n",
    "        global_mean = df[column].mean()\n",
    "        global_max = df[column].max()\n",
    "        global_min = df[column].min()\n",
    "\n",
    "        df[column] = df[column].fillna(global_mean)\n",
    "        df[f'{column}_maxdiff'] = df[f'{column}_maxdiff'].fillna(global_max - df[column])\n",
    "        df[f'{column}_mindiff'] = df[f'{column}_mindiff'].fillna(df[column] - global_min)\n",
    "        df[f'{column}_meandiff'] = df[f'{column}_meandiff'].fillna(df[column] - global_mean)\n",
    "\n",
    "    # Standardize columns by subtracting the mean and dividing by the standard deviation within each group\n",
    "    # Adding a small value (epsilon) to the standard deviation to avoid division by zero\n",
    "    for column in columns_to_impute:\n",
    "        df[column] = df.groupby('id')[column].transform(lambda x: (x - x.mean()) / (x.std() + 1e-8))\n",
    "\n",
    "    # Fill practiced_sport with 0\n",
    "    df['practiced_sport'] = df['practiced_sport'].fillna(0)\n",
    "\n",
    "    return df\n",
    "\n",
    "# def impute_columns_mean(df):\n",
    "#     columns_to_impute = ['steps_total', 'distance', 'sleep', 'app_usage', 'home_cluster', 't_sport', 'clusters_count']\n",
    "\n",
    "#     for column in columns_to_impute:\n",
    "#         # First, impute missing values with the mean within each group\n",
    "#         df[column] = df.groupby('id')[column].transform(lambda x: x.fillna(x.mean()))\n",
    "#         # Then, for any remaining NaN values, impute with the global column mean\n",
    "#         global_mean = df[column].mean()\n",
    "#         df[column] = df[column].fillna(global_mean)\n",
    "\n",
    "#     # Standardize columns by subtracting the mean and dividing by the standard deviation within each group\n",
    "#     # Adding a small value (epsilon) to the standard deviation to avoid division by zero\n",
    "#     for column in columns_to_impute:\n",
    "#         df[column] = df.groupby('id')[column].transform(lambda x: (x - x.mean()) / (x.std() + 1e-8))\n",
    "\n",
    "#     # Fill practiced_sport with 0\n",
    "#     df['practiced_sport'] = df['practiced_sport'].fillna(0)\n",
    "\n",
    "#     return df\n",
    "\n",
    "\n",
    "def impute_columns_mask(df):\n",
    "    columns_to_impute = ['steps_total', 'distance', 'sleep', 'app_usage', 'home_cluster', 't_sport', 'clusters_count']  # Add other columns as needed\n",
    "    \n",
    "    for column in columns_to_impute:\n",
    "        df[column] = df.groupby('id')[column].transform(lambda x: (x - x.mean()) / x.std())\n",
    "        df[f'{column}_maxdiff'] = df.groupby('id')[column].transform(lambda x: x.max() - x)\n",
    "        df[f'{column}_mindiff'] = df.groupby('id')[column].transform(lambda x: x - x.min())\n",
    "        df[f'{column}_meandiff'] = df.groupby('id')[column].transform(lambda x: x - x.mean())\n",
    "    \n",
    "    new_columns = [f'{column}_maxdiff' for column in columns_to_impute] + [f'{column}_mindiff' for column in columns_to_impute] + [f'{column}_meandiff' for column in columns_to_impute]\n",
    "    new_columns.append(columns_to_impute)\n",
    "    \n",
    "    new_columns.append('practiced_sport')\n",
    "    \n",
    "    for column in new_columns:\n",
    "        df[column] = df.groupby('id')[column].transform(lambda x: x.fillna(-9999.0))\n",
    "\n",
    "    return df\n",
    "\n",
    "#df = impute_columns_mean(df)\n",
    "df = impute_columns_mask(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                              0\n",
       "date                            0\n",
       "is_weekend                      0\n",
       "steps_total                     0\n",
       "distance                        0\n",
       "sleep                           0\n",
       "app_usage                       0\n",
       "home_cluster                    0\n",
       "clusters_count                  0\n",
       "t_sport                         0\n",
       "practiced_sport                 0\n",
       "valence                    100994\n",
       "steps_total_maxdiff             0\n",
       "steps_total_mindiff             0\n",
       "steps_total_meandiff            0\n",
       "distance_maxdiff                0\n",
       "distance_mindiff                0\n",
       "distance_meandiff               0\n",
       "sleep_maxdiff                   0\n",
       "sleep_mindiff                   0\n",
       "sleep_meandiff                  0\n",
       "app_usage_maxdiff               0\n",
       "app_usage_mindiff               0\n",
       "app_usage_meandiff              0\n",
       "home_cluster_maxdiff            0\n",
       "home_cluster_mindiff            0\n",
       "home_cluster_meandiff           0\n",
       "t_sport_maxdiff                 0\n",
       "t_sport_mindiff                 0\n",
       "t_sport_meandiff                0\n",
       "clusters_count_maxdiff          0\n",
       "clusters_count_mindiff          0\n",
       "clusters_count_meandiff         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# count nans\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>is_weekend</th>\n",
       "      <th>steps_total</th>\n",
       "      <th>distance</th>\n",
       "      <th>sleep</th>\n",
       "      <th>app_usage</th>\n",
       "      <th>home_cluster</th>\n",
       "      <th>clusters_count</th>\n",
       "      <th>t_sport</th>\n",
       "      <th>...</th>\n",
       "      <th>app_usage_meandiff</th>\n",
       "      <th>home_cluster_maxdiff</th>\n",
       "      <th>home_cluster_mindiff</th>\n",
       "      <th>home_cluster_meandiff</th>\n",
       "      <th>t_sport_maxdiff</th>\n",
       "      <th>t_sport_mindiff</th>\n",
       "      <th>t_sport_meandiff</th>\n",
       "      <th>clusters_count_maxdiff</th>\n",
       "      <th>clusters_count_mindiff</th>\n",
       "      <th>clusters_count_meandiff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>2019-05-31</td>\n",
       "      <td>0</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>-2.366512</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.366512</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>-9999.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>2019-06-01</td>\n",
       "      <td>1</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>0.608229</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.608229</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>-9999.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>2019-06-02</td>\n",
       "      <td>1</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>-0.709614</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.709614</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>-9999.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>2019-06-03</td>\n",
       "      <td>0</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>-0.487415</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.487415</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>-9999.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>2019-06-04</td>\n",
       "      <td>0</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>0.873409</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.873409</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>-9999.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>2019-06-05</td>\n",
       "      <td>0</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>1.282117</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.282117</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>-9999.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>5</td>\n",
       "      <td>2019-06-06</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.594180</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>-0.544596</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.544596</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>-9999.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>5</td>\n",
       "      <td>2019-06-07</td>\n",
       "      <td>0</td>\n",
       "      <td>0.436693</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>-1.377489</td>\n",
       "      <td>0.963018</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>-0.086058</td>\n",
       "      <td>...</td>\n",
       "      <td>0.963018</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>2.442272</td>\n",
       "      <td>0.940946</td>\n",
       "      <td>-0.086058</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>-9999.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>5</td>\n",
       "      <td>2019-06-08</td>\n",
       "      <td>1</td>\n",
       "      <td>1.054521</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>-0.977218</td>\n",
       "      <td>0.551624</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>-0.398886</td>\n",
       "      <td>...</td>\n",
       "      <td>0.551624</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>2.755100</td>\n",
       "      <td>0.628118</td>\n",
       "      <td>-0.398886</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>-9999.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>5</td>\n",
       "      <td>2019-06-09</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.773117</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>-0.661216</td>\n",
       "      <td>-1.368727</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.368727</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>-9999.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>5</td>\n",
       "      <td>2019-06-10</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.472901</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>-0.615641</td>\n",
       "      <td>-0.254855</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.254855</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>-9999.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>5</td>\n",
       "      <td>2019-06-11</td>\n",
       "      <td>0</td>\n",
       "      <td>4.273887</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>0.620000</td>\n",
       "      <td>-0.804212</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>1.960869</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.804212</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>0.395345</td>\n",
       "      <td>2.987873</td>\n",
       "      <td>1.960869</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>-9999.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>5</td>\n",
       "      <td>2019-06-12</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.079738</td>\n",
       "      <td>-0.538504</td>\n",
       "      <td>0.336230</td>\n",
       "      <td>-0.892093</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-0.575106</td>\n",
       "      <td>-0.949413</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.892093</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>3.305627</td>\n",
       "      <td>0.077591</td>\n",
       "      <td>-0.949413</td>\n",
       "      <td>3.587972</td>\n",
       "      <td>0.815448</td>\n",
       "      <td>-0.575106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>5</td>\n",
       "      <td>2019-06-13</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.214437</td>\n",
       "      <td>-0.396745</td>\n",
       "      <td>1.827342</td>\n",
       "      <td>0.131211</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>0.077253</td>\n",
       "      <td>0.044492</td>\n",
       "      <td>...</td>\n",
       "      <td>0.131211</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>2.311722</td>\n",
       "      <td>1.071496</td>\n",
       "      <td>0.044492</td>\n",
       "      <td>2.935613</td>\n",
       "      <td>1.467807</td>\n",
       "      <td>0.077253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>5</td>\n",
       "      <td>2019-06-14</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.277562</td>\n",
       "      <td>-0.300927</td>\n",
       "      <td>0.325556</td>\n",
       "      <td>-0.217630</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>1.381970</td>\n",
       "      <td>-1.018383</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.217630</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>3.374597</td>\n",
       "      <td>0.008621</td>\n",
       "      <td>-1.018383</td>\n",
       "      <td>1.630896</td>\n",
       "      <td>2.772524</td>\n",
       "      <td>1.381970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>5</td>\n",
       "      <td>2019-06-15</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.409776</td>\n",
       "      <td>1.172920</td>\n",
       "      <td>2.273818</td>\n",
       "      <td>-0.267519</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>0.403432</td>\n",
       "      <td>-0.209219</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.267519</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>2.565433</td>\n",
       "      <td>0.817786</td>\n",
       "      <td>-0.209219</td>\n",
       "      <td>2.609434</td>\n",
       "      <td>1.793986</td>\n",
       "      <td>0.403432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>5</td>\n",
       "      <td>2019-06-16</td>\n",
       "      <td>1</td>\n",
       "      <td>1.175800</td>\n",
       "      <td>1.614273</td>\n",
       "      <td>0.388686</td>\n",
       "      <td>0.064436</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>0.566522</td>\n",
       "      <td>0.576545</td>\n",
       "      <td>...</td>\n",
       "      <td>0.064436</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>1.779669</td>\n",
       "      <td>1.603549</td>\n",
       "      <td>0.576545</td>\n",
       "      <td>2.446344</td>\n",
       "      <td>1.957076</td>\n",
       "      <td>0.566522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>5</td>\n",
       "      <td>2019-06-17</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.450037</td>\n",
       "      <td>-0.479696</td>\n",
       "      <td>0.142134</td>\n",
       "      <td>-1.229805</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>0.403432</td>\n",
       "      <td>2.356214</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.229805</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.383218</td>\n",
       "      <td>2.356214</td>\n",
       "      <td>2.609434</td>\n",
       "      <td>1.793986</td>\n",
       "      <td>0.403432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>5</td>\n",
       "      <td>2019-06-18</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.200023</td>\n",
       "      <td>-0.345467</td>\n",
       "      <td>-1.604589</td>\n",
       "      <td>2.767088</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>0.729611</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>2.767088</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>2.283255</td>\n",
       "      <td>2.120165</td>\n",
       "      <td>0.729611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>5</td>\n",
       "      <td>2019-06-19</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.227857</td>\n",
       "      <td>-0.412266</td>\n",
       "      <td>-1.215133</td>\n",
       "      <td>0.101469</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-1.227464</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.101469</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>4.240330</td>\n",
       "      <td>0.163090</td>\n",
       "      <td>-1.227464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>5</td>\n",
       "      <td>2019-06-20</td>\n",
       "      <td>0</td>\n",
       "      <td>0.385497</td>\n",
       "      <td>-0.155968</td>\n",
       "      <td>0.914374</td>\n",
       "      <td>0.586930</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>3.012866</td>\n",
       "      <td>-1.027004</td>\n",
       "      <td>...</td>\n",
       "      <td>0.586930</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>3.383218</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.027004</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.403420</td>\n",
       "      <td>3.012866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>5</td>\n",
       "      <td>2019-06-21</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.456995</td>\n",
       "      <td>-0.460682</td>\n",
       "      <td>0.658833</td>\n",
       "      <td>-0.241423</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-0.248926</td>\n",
       "      <td>-0.257251</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.241423</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>2.613465</td>\n",
       "      <td>0.769753</td>\n",
       "      <td>-0.257251</td>\n",
       "      <td>3.261793</td>\n",
       "      <td>1.141627</td>\n",
       "      <td>-0.248926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>5</td>\n",
       "      <td>2019-06-22</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.211952</td>\n",
       "      <td>3.520942</td>\n",
       "      <td>-0.594715</td>\n",
       "      <td>-0.661068</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-0.575106</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.661068</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>3.587972</td>\n",
       "      <td>0.815448</td>\n",
       "      <td>-0.575106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>5</td>\n",
       "      <td>2019-06-23</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.769140</td>\n",
       "      <td>-0.449564</td>\n",
       "      <td>0.865991</td>\n",
       "      <td>0.346118</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-1.390554</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.346118</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>4.403420</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.390554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>5</td>\n",
       "      <td>2019-06-24</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.126957</td>\n",
       "      <td>-0.076575</td>\n",
       "      <td>-0.132648</td>\n",
       "      <td>-0.432729</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-0.412016</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.432729</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>3.424882</td>\n",
       "      <td>0.978538</td>\n",
       "      <td>-0.412016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>5</td>\n",
       "      <td>2019-06-25</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.249727</td>\n",
       "      <td>-0.265404</td>\n",
       "      <td>-1.362320</td>\n",
       "      <td>-0.160449</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-0.248926</td>\n",
       "      <td>0.394267</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.160449</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>1.961946</td>\n",
       "      <td>1.421272</td>\n",
       "      <td>0.394267</td>\n",
       "      <td>3.261793</td>\n",
       "      <td>1.141627</td>\n",
       "      <td>-0.248926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>5</td>\n",
       "      <td>2019-06-26</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.310864</td>\n",
       "      <td>-0.375079</td>\n",
       "      <td>-1.533243</td>\n",
       "      <td>-0.308773</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-0.901285</td>\n",
       "      <td>-0.917392</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.308773</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>3.273605</td>\n",
       "      <td>0.109613</td>\n",
       "      <td>-0.917392</td>\n",
       "      <td>3.914151</td>\n",
       "      <td>0.489269</td>\n",
       "      <td>-0.901285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>5</td>\n",
       "      <td>2019-06-27</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.370509</td>\n",
       "      <td>-0.337202</td>\n",
       "      <td>0.681656</td>\n",
       "      <td>0.814693</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-0.248926</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.814693</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>3.261793</td>\n",
       "      <td>1.141627</td>\n",
       "      <td>-0.248926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>5</td>\n",
       "      <td>2019-06-28</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.326272</td>\n",
       "      <td>-0.257014</td>\n",
       "      <td>0.109410</td>\n",
       "      <td>1.776979</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>0.240343</td>\n",
       "      <td>0.199674</td>\n",
       "      <td>...</td>\n",
       "      <td>1.776979</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>2.156540</td>\n",
       "      <td>1.226678</td>\n",
       "      <td>0.199674</td>\n",
       "      <td>2.772524</td>\n",
       "      <td>1.630896</td>\n",
       "      <td>0.240343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>5</td>\n",
       "      <td>2019-06-29</td>\n",
       "      <td>1</td>\n",
       "      <td>0.301994</td>\n",
       "      <td>-0.398457</td>\n",
       "      <td>0.604692</td>\n",
       "      <td>0.830811</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-0.575106</td>\n",
       "      <td>0.273570</td>\n",
       "      <td>...</td>\n",
       "      <td>0.830811</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>2.082644</td>\n",
       "      <td>1.300575</td>\n",
       "      <td>0.273570</td>\n",
       "      <td>3.587972</td>\n",
       "      <td>0.815448</td>\n",
       "      <td>-0.575106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>5</td>\n",
       "      <td>2019-06-30</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.841709</td>\n",
       "      <td>-0.460385</td>\n",
       "      <td>0.547811</td>\n",
       "      <td>0.516701</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-0.412016</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.516701</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>3.424882</td>\n",
       "      <td>0.978538</td>\n",
       "      <td>-0.412016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>5</td>\n",
       "      <td>2019-07-01</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.264639</td>\n",
       "      <td>-0.598201</td>\n",
       "      <td>0.228508</td>\n",
       "      <td>-1.267414</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>-0.942024</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.267414</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>3.298237</td>\n",
       "      <td>0.084981</td>\n",
       "      <td>-0.942024</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>-9999.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>5</td>\n",
       "      <td>2019-07-02</td>\n",
       "      <td>0</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>-0.450828</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>-9999.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>16</td>\n",
       "      <td>2019-07-09</td>\n",
       "      <td>0</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>0.759636</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.759636</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>-9999.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>16</td>\n",
       "      <td>2019-07-10</td>\n",
       "      <td>0</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>0.494982</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.494982</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>-9999.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>16</td>\n",
       "      <td>2019-07-11</td>\n",
       "      <td>0</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>2.292801</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>2.292801</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>-9999.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>16</td>\n",
       "      <td>2019-07-12</td>\n",
       "      <td>0</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>1.815485</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.815485</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>-9999.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>16</td>\n",
       "      <td>2019-07-13</td>\n",
       "      <td>1</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>2.960657</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>2.960657</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>-9999.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>16</td>\n",
       "      <td>2019-07-14</td>\n",
       "      <td>1</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>1.660066</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.660066</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>-9999.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>16</td>\n",
       "      <td>2019-07-15</td>\n",
       "      <td>0</td>\n",
       "      <td>1.345164</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>0.274023</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>-0.443317</td>\n",
       "      <td>...</td>\n",
       "      <td>0.274023</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>2.711890</td>\n",
       "      <td>0.388642</td>\n",
       "      <td>-0.443317</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>-9999.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>16</td>\n",
       "      <td>2019-07-16</td>\n",
       "      <td>0</td>\n",
       "      <td>2.231180</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>-0.212543</td>\n",
       "      <td>0.921968</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>2.268572</td>\n",
       "      <td>...</td>\n",
       "      <td>0.921968</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.100532</td>\n",
       "      <td>2.268572</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>-9999.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>16</td>\n",
       "      <td>2019-07-17</td>\n",
       "      <td>0</td>\n",
       "      <td>1.860871</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>-0.099753</td>\n",
       "      <td>1.653706</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>0.117417</td>\n",
       "      <td>...</td>\n",
       "      <td>1.653706</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>2.151155</td>\n",
       "      <td>0.949376</td>\n",
       "      <td>0.117417</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>-9999.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>16</td>\n",
       "      <td>2019-07-18</td>\n",
       "      <td>0</td>\n",
       "      <td>2.193468</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>-2.033942</td>\n",
       "      <td>1.520135</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>-0.831959</td>\n",
       "      <td>...</td>\n",
       "      <td>1.520135</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>3.100532</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.831959</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>-9999.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>16</td>\n",
       "      <td>2019-07-19</td>\n",
       "      <td>0</td>\n",
       "      <td>2.723260</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>0.529172</td>\n",
       "      <td>0.873572</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>0.247921</td>\n",
       "      <td>...</td>\n",
       "      <td>0.873572</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>2.020652</td>\n",
       "      <td>1.079880</td>\n",
       "      <td>0.247921</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>-9999.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>16</td>\n",
       "      <td>2019-07-20</td>\n",
       "      <td>1</td>\n",
       "      <td>1.478294</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>0.388738</td>\n",
       "      <td>-0.326633</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.326633</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>-9999.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>16</td>\n",
       "      <td>2019-07-21</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.019755</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>-0.903210</td>\n",
       "      <td>0.560800</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>-0.471999</td>\n",
       "      <td>...</td>\n",
       "      <td>0.560800</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>2.740572</td>\n",
       "      <td>0.359960</td>\n",
       "      <td>-0.471999</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>-9999.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>16</td>\n",
       "      <td>2019-07-22</td>\n",
       "      <td>0</td>\n",
       "      <td>2.060793</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>-0.897056</td>\n",
       "      <td>0.783972</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>-0.073319</td>\n",
       "      <td>...</td>\n",
       "      <td>0.783972</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>2.341891</td>\n",
       "      <td>0.758641</td>\n",
       "      <td>-0.073319</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>-9999.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>16</td>\n",
       "      <td>2019-07-23</td>\n",
       "      <td>0</td>\n",
       "      <td>2.530154</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>-0.956579</td>\n",
       "      <td>0.153726</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>-0.813316</td>\n",
       "      <td>...</td>\n",
       "      <td>0.153726</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>3.081888</td>\n",
       "      <td>0.018643</td>\n",
       "      <td>-0.813316</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>-9999.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>16</td>\n",
       "      <td>2019-07-24</td>\n",
       "      <td>0</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>-0.927624</td>\n",
       "      <td>0.343989</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.343989</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>-9999.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>16</td>\n",
       "      <td>2019-07-25</td>\n",
       "      <td>0</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>0.504657</td>\n",
       "      <td>1.519581</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.519581</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>-9999.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50 rows × 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    id        date  is_weekend  steps_total     distance        sleep  \\\n",
       "0    5  2019-05-31           0 -9999.000000 -9999.000000 -9999.000000   \n",
       "1    5  2019-06-01           1 -9999.000000 -9999.000000 -9999.000000   \n",
       "2    5  2019-06-02           1 -9999.000000 -9999.000000 -9999.000000   \n",
       "3    5  2019-06-03           0 -9999.000000 -9999.000000 -9999.000000   \n",
       "4    5  2019-06-04           0 -9999.000000 -9999.000000 -9999.000000   \n",
       "5    5  2019-06-05           0 -9999.000000 -9999.000000 -9999.000000   \n",
       "6    5  2019-06-06           0    -0.594180 -9999.000000 -9999.000000   \n",
       "7    5  2019-06-07           0     0.436693 -9999.000000    -1.377489   \n",
       "8    5  2019-06-08           1     1.054521 -9999.000000    -0.977218   \n",
       "9    5  2019-06-09           1    -0.773117 -9999.000000    -0.661216   \n",
       "10   5  2019-06-10           0    -0.472901 -9999.000000    -0.615641   \n",
       "11   5  2019-06-11           0     4.273887 -9999.000000     0.620000   \n",
       "12   5  2019-06-12           0    -0.079738    -0.538504     0.336230   \n",
       "13   5  2019-06-13           0    -0.214437    -0.396745     1.827342   \n",
       "14   5  2019-06-14           0    -0.277562    -0.300927     0.325556   \n",
       "15   5  2019-06-15           1    -0.409776     1.172920     2.273818   \n",
       "16   5  2019-06-16           1     1.175800     1.614273     0.388686   \n",
       "17   5  2019-06-17           0    -0.450037    -0.479696     0.142134   \n",
       "18   5  2019-06-18           0    -0.200023    -0.345467    -1.604589   \n",
       "19   5  2019-06-19           0    -0.227857    -0.412266    -1.215133   \n",
       "20   5  2019-06-20           0     0.385497    -0.155968     0.914374   \n",
       "21   5  2019-06-21           0    -0.456995    -0.460682     0.658833   \n",
       "22   5  2019-06-22           1    -0.211952     3.520942    -0.594715   \n",
       "23   5  2019-06-23           1    -0.769140    -0.449564     0.865991   \n",
       "24   5  2019-06-24           0    -0.126957    -0.076575    -0.132648   \n",
       "25   5  2019-06-25           0    -0.249727    -0.265404    -1.362320   \n",
       "26   5  2019-06-26           0    -0.310864    -0.375079    -1.533243   \n",
       "27   5  2019-06-27           0    -0.370509    -0.337202     0.681656   \n",
       "28   5  2019-06-28           0    -0.326272    -0.257014     0.109410   \n",
       "29   5  2019-06-29           1     0.301994    -0.398457     0.604692   \n",
       "30   5  2019-06-30           1    -0.841709    -0.460385     0.547811   \n",
       "31   5  2019-07-01           0    -0.264639    -0.598201     0.228508   \n",
       "32   5  2019-07-02           0 -9999.000000 -9999.000000    -0.450828   \n",
       "33  16  2019-07-09           0 -9999.000000 -9999.000000 -9999.000000   \n",
       "34  16  2019-07-10           0 -9999.000000 -9999.000000 -9999.000000   \n",
       "35  16  2019-07-11           0 -9999.000000 -9999.000000 -9999.000000   \n",
       "36  16  2019-07-12           0 -9999.000000 -9999.000000 -9999.000000   \n",
       "37  16  2019-07-13           1 -9999.000000 -9999.000000 -9999.000000   \n",
       "38  16  2019-07-14           1 -9999.000000 -9999.000000 -9999.000000   \n",
       "39  16  2019-07-15           0     1.345164 -9999.000000 -9999.000000   \n",
       "40  16  2019-07-16           0     2.231180 -9999.000000    -0.212543   \n",
       "41  16  2019-07-17           0     1.860871 -9999.000000    -0.099753   \n",
       "42  16  2019-07-18           0     2.193468 -9999.000000    -2.033942   \n",
       "43  16  2019-07-19           0     2.723260 -9999.000000     0.529172   \n",
       "44  16  2019-07-20           1     1.478294 -9999.000000     0.388738   \n",
       "45  16  2019-07-21           1    -0.019755 -9999.000000    -0.903210   \n",
       "46  16  2019-07-22           0     2.060793 -9999.000000    -0.897056   \n",
       "47  16  2019-07-23           0     2.530154 -9999.000000    -0.956579   \n",
       "48  16  2019-07-24           0 -9999.000000 -9999.000000    -0.927624   \n",
       "49  16  2019-07-25           0 -9999.000000 -9999.000000     0.504657   \n",
       "\n",
       "      app_usage  home_cluster  clusters_count      t_sport  ...  \\\n",
       "0     -2.366512       -9999.0    -9999.000000 -9999.000000  ...   \n",
       "1      0.608229       -9999.0    -9999.000000 -9999.000000  ...   \n",
       "2     -0.709614       -9999.0    -9999.000000 -9999.000000  ...   \n",
       "3     -0.487415       -9999.0    -9999.000000 -9999.000000  ...   \n",
       "4      0.873409       -9999.0    -9999.000000 -9999.000000  ...   \n",
       "5      1.282117       -9999.0    -9999.000000 -9999.000000  ...   \n",
       "6     -0.544596       -9999.0    -9999.000000 -9999.000000  ...   \n",
       "7      0.963018       -9999.0    -9999.000000    -0.086058  ...   \n",
       "8      0.551624       -9999.0    -9999.000000    -0.398886  ...   \n",
       "9     -1.368727       -9999.0    -9999.000000 -9999.000000  ...   \n",
       "10    -0.254855       -9999.0    -9999.000000 -9999.000000  ...   \n",
       "11    -0.804212       -9999.0    -9999.000000     1.960869  ...   \n",
       "12    -0.892093       -9999.0       -0.575106    -0.949413  ...   \n",
       "13     0.131211       -9999.0        0.077253     0.044492  ...   \n",
       "14    -0.217630       -9999.0        1.381970    -1.018383  ...   \n",
       "15    -0.267519       -9999.0        0.403432    -0.209219  ...   \n",
       "16     0.064436       -9999.0        0.566522     0.576545  ...   \n",
       "17    -1.229805       -9999.0        0.403432     2.356214  ...   \n",
       "18     2.767088       -9999.0        0.729611 -9999.000000  ...   \n",
       "19     0.101469       -9999.0       -1.227464 -9999.000000  ...   \n",
       "20     0.586930       -9999.0        3.012866    -1.027004  ...   \n",
       "21    -0.241423       -9999.0       -0.248926    -0.257251  ...   \n",
       "22    -0.661068       -9999.0       -0.575106 -9999.000000  ...   \n",
       "23     0.346118       -9999.0       -1.390554 -9999.000000  ...   \n",
       "24    -0.432729       -9999.0       -0.412016 -9999.000000  ...   \n",
       "25    -0.160449       -9999.0       -0.248926     0.394267  ...   \n",
       "26    -0.308773       -9999.0       -0.901285    -0.917392  ...   \n",
       "27     0.814693       -9999.0       -0.248926 -9999.000000  ...   \n",
       "28     1.776979       -9999.0        0.240343     0.199674  ...   \n",
       "29     0.830811       -9999.0       -0.575106     0.273570  ...   \n",
       "30     0.516701       -9999.0       -0.412016 -9999.000000  ...   \n",
       "31    -1.267414       -9999.0    -9999.000000    -0.942024  ...   \n",
       "32 -9999.000000       -9999.0    -9999.000000 -9999.000000  ...   \n",
       "33     0.759636       -9999.0    -9999.000000 -9999.000000  ...   \n",
       "34     0.494982       -9999.0    -9999.000000 -9999.000000  ...   \n",
       "35     2.292801       -9999.0    -9999.000000 -9999.000000  ...   \n",
       "36     1.815485       -9999.0    -9999.000000 -9999.000000  ...   \n",
       "37     2.960657       -9999.0    -9999.000000 -9999.000000  ...   \n",
       "38     1.660066       -9999.0    -9999.000000 -9999.000000  ...   \n",
       "39     0.274023       -9999.0    -9999.000000    -0.443317  ...   \n",
       "40     0.921968       -9999.0    -9999.000000     2.268572  ...   \n",
       "41     1.653706       -9999.0    -9999.000000     0.117417  ...   \n",
       "42     1.520135       -9999.0    -9999.000000    -0.831959  ...   \n",
       "43     0.873572       -9999.0    -9999.000000     0.247921  ...   \n",
       "44    -0.326633       -9999.0    -9999.000000 -9999.000000  ...   \n",
       "45     0.560800       -9999.0    -9999.000000    -0.471999  ...   \n",
       "46     0.783972       -9999.0    -9999.000000    -0.073319  ...   \n",
       "47     0.153726       -9999.0    -9999.000000    -0.813316  ...   \n",
       "48     0.343989       -9999.0    -9999.000000 -9999.000000  ...   \n",
       "49     1.519581       -9999.0    -9999.000000 -9999.000000  ...   \n",
       "\n",
       "    app_usage_meandiff  home_cluster_maxdiff  home_cluster_mindiff  \\\n",
       "0            -2.366512               -9999.0               -9999.0   \n",
       "1             0.608229               -9999.0               -9999.0   \n",
       "2            -0.709614               -9999.0               -9999.0   \n",
       "3            -0.487415               -9999.0               -9999.0   \n",
       "4             0.873409               -9999.0               -9999.0   \n",
       "5             1.282117               -9999.0               -9999.0   \n",
       "6            -0.544596               -9999.0               -9999.0   \n",
       "7             0.963018               -9999.0               -9999.0   \n",
       "8             0.551624               -9999.0               -9999.0   \n",
       "9            -1.368727               -9999.0               -9999.0   \n",
       "10           -0.254855               -9999.0               -9999.0   \n",
       "11           -0.804212               -9999.0               -9999.0   \n",
       "12           -0.892093               -9999.0               -9999.0   \n",
       "13            0.131211               -9999.0               -9999.0   \n",
       "14           -0.217630               -9999.0               -9999.0   \n",
       "15           -0.267519               -9999.0               -9999.0   \n",
       "16            0.064436               -9999.0               -9999.0   \n",
       "17           -1.229805               -9999.0               -9999.0   \n",
       "18            2.767088               -9999.0               -9999.0   \n",
       "19            0.101469               -9999.0               -9999.0   \n",
       "20            0.586930               -9999.0               -9999.0   \n",
       "21           -0.241423               -9999.0               -9999.0   \n",
       "22           -0.661068               -9999.0               -9999.0   \n",
       "23            0.346118               -9999.0               -9999.0   \n",
       "24           -0.432729               -9999.0               -9999.0   \n",
       "25           -0.160449               -9999.0               -9999.0   \n",
       "26           -0.308773               -9999.0               -9999.0   \n",
       "27            0.814693               -9999.0               -9999.0   \n",
       "28            1.776979               -9999.0               -9999.0   \n",
       "29            0.830811               -9999.0               -9999.0   \n",
       "30            0.516701               -9999.0               -9999.0   \n",
       "31           -1.267414               -9999.0               -9999.0   \n",
       "32        -9999.000000               -9999.0               -9999.0   \n",
       "33            0.759636               -9999.0               -9999.0   \n",
       "34            0.494982               -9999.0               -9999.0   \n",
       "35            2.292801               -9999.0               -9999.0   \n",
       "36            1.815485               -9999.0               -9999.0   \n",
       "37            2.960657               -9999.0               -9999.0   \n",
       "38            1.660066               -9999.0               -9999.0   \n",
       "39            0.274023               -9999.0               -9999.0   \n",
       "40            0.921968               -9999.0               -9999.0   \n",
       "41            1.653706               -9999.0               -9999.0   \n",
       "42            1.520135               -9999.0               -9999.0   \n",
       "43            0.873572               -9999.0               -9999.0   \n",
       "44           -0.326633               -9999.0               -9999.0   \n",
       "45            0.560800               -9999.0               -9999.0   \n",
       "46            0.783972               -9999.0               -9999.0   \n",
       "47            0.153726               -9999.0               -9999.0   \n",
       "48            0.343989               -9999.0               -9999.0   \n",
       "49            1.519581               -9999.0               -9999.0   \n",
       "\n",
       "    home_cluster_meandiff  t_sport_maxdiff  t_sport_mindiff  t_sport_meandiff  \\\n",
       "0                 -9999.0     -9999.000000     -9999.000000      -9999.000000   \n",
       "1                 -9999.0     -9999.000000     -9999.000000      -9999.000000   \n",
       "2                 -9999.0     -9999.000000     -9999.000000      -9999.000000   \n",
       "3                 -9999.0     -9999.000000     -9999.000000      -9999.000000   \n",
       "4                 -9999.0     -9999.000000     -9999.000000      -9999.000000   \n",
       "5                 -9999.0     -9999.000000     -9999.000000      -9999.000000   \n",
       "6                 -9999.0     -9999.000000     -9999.000000      -9999.000000   \n",
       "7                 -9999.0         2.442272         0.940946         -0.086058   \n",
       "8                 -9999.0         2.755100         0.628118         -0.398886   \n",
       "9                 -9999.0     -9999.000000     -9999.000000      -9999.000000   \n",
       "10                -9999.0     -9999.000000     -9999.000000      -9999.000000   \n",
       "11                -9999.0         0.395345         2.987873          1.960869   \n",
       "12                -9999.0         3.305627         0.077591         -0.949413   \n",
       "13                -9999.0         2.311722         1.071496          0.044492   \n",
       "14                -9999.0         3.374597         0.008621         -1.018383   \n",
       "15                -9999.0         2.565433         0.817786         -0.209219   \n",
       "16                -9999.0         1.779669         1.603549          0.576545   \n",
       "17                -9999.0         0.000000         3.383218          2.356214   \n",
       "18                -9999.0     -9999.000000     -9999.000000      -9999.000000   \n",
       "19                -9999.0     -9999.000000     -9999.000000      -9999.000000   \n",
       "20                -9999.0         3.383218         0.000000         -1.027004   \n",
       "21                -9999.0         2.613465         0.769753         -0.257251   \n",
       "22                -9999.0     -9999.000000     -9999.000000      -9999.000000   \n",
       "23                -9999.0     -9999.000000     -9999.000000      -9999.000000   \n",
       "24                -9999.0     -9999.000000     -9999.000000      -9999.000000   \n",
       "25                -9999.0         1.961946         1.421272          0.394267   \n",
       "26                -9999.0         3.273605         0.109613         -0.917392   \n",
       "27                -9999.0     -9999.000000     -9999.000000      -9999.000000   \n",
       "28                -9999.0         2.156540         1.226678          0.199674   \n",
       "29                -9999.0         2.082644         1.300575          0.273570   \n",
       "30                -9999.0     -9999.000000     -9999.000000      -9999.000000   \n",
       "31                -9999.0         3.298237         0.084981         -0.942024   \n",
       "32                -9999.0     -9999.000000     -9999.000000      -9999.000000   \n",
       "33                -9999.0     -9999.000000     -9999.000000      -9999.000000   \n",
       "34                -9999.0     -9999.000000     -9999.000000      -9999.000000   \n",
       "35                -9999.0     -9999.000000     -9999.000000      -9999.000000   \n",
       "36                -9999.0     -9999.000000     -9999.000000      -9999.000000   \n",
       "37                -9999.0     -9999.000000     -9999.000000      -9999.000000   \n",
       "38                -9999.0     -9999.000000     -9999.000000      -9999.000000   \n",
       "39                -9999.0         2.711890         0.388642         -0.443317   \n",
       "40                -9999.0         0.000000         3.100532          2.268572   \n",
       "41                -9999.0         2.151155         0.949376          0.117417   \n",
       "42                -9999.0         3.100532         0.000000         -0.831959   \n",
       "43                -9999.0         2.020652         1.079880          0.247921   \n",
       "44                -9999.0     -9999.000000     -9999.000000      -9999.000000   \n",
       "45                -9999.0         2.740572         0.359960         -0.471999   \n",
       "46                -9999.0         2.341891         0.758641         -0.073319   \n",
       "47                -9999.0         3.081888         0.018643         -0.813316   \n",
       "48                -9999.0     -9999.000000     -9999.000000      -9999.000000   \n",
       "49                -9999.0     -9999.000000     -9999.000000      -9999.000000   \n",
       "\n",
       "    clusters_count_maxdiff  clusters_count_mindiff  clusters_count_meandiff  \n",
       "0             -9999.000000            -9999.000000             -9999.000000  \n",
       "1             -9999.000000            -9999.000000             -9999.000000  \n",
       "2             -9999.000000            -9999.000000             -9999.000000  \n",
       "3             -9999.000000            -9999.000000             -9999.000000  \n",
       "4             -9999.000000            -9999.000000             -9999.000000  \n",
       "5             -9999.000000            -9999.000000             -9999.000000  \n",
       "6             -9999.000000            -9999.000000             -9999.000000  \n",
       "7             -9999.000000            -9999.000000             -9999.000000  \n",
       "8             -9999.000000            -9999.000000             -9999.000000  \n",
       "9             -9999.000000            -9999.000000             -9999.000000  \n",
       "10            -9999.000000            -9999.000000             -9999.000000  \n",
       "11            -9999.000000            -9999.000000             -9999.000000  \n",
       "12                3.587972                0.815448                -0.575106  \n",
       "13                2.935613                1.467807                 0.077253  \n",
       "14                1.630896                2.772524                 1.381970  \n",
       "15                2.609434                1.793986                 0.403432  \n",
       "16                2.446344                1.957076                 0.566522  \n",
       "17                2.609434                1.793986                 0.403432  \n",
       "18                2.283255                2.120165                 0.729611  \n",
       "19                4.240330                0.163090                -1.227464  \n",
       "20                0.000000                4.403420                 3.012866  \n",
       "21                3.261793                1.141627                -0.248926  \n",
       "22                3.587972                0.815448                -0.575106  \n",
       "23                4.403420                0.000000                -1.390554  \n",
       "24                3.424882                0.978538                -0.412016  \n",
       "25                3.261793                1.141627                -0.248926  \n",
       "26                3.914151                0.489269                -0.901285  \n",
       "27                3.261793                1.141627                -0.248926  \n",
       "28                2.772524                1.630896                 0.240343  \n",
       "29                3.587972                0.815448                -0.575106  \n",
       "30                3.424882                0.978538                -0.412016  \n",
       "31            -9999.000000            -9999.000000             -9999.000000  \n",
       "32            -9999.000000            -9999.000000             -9999.000000  \n",
       "33            -9999.000000            -9999.000000             -9999.000000  \n",
       "34            -9999.000000            -9999.000000             -9999.000000  \n",
       "35            -9999.000000            -9999.000000             -9999.000000  \n",
       "36            -9999.000000            -9999.000000             -9999.000000  \n",
       "37            -9999.000000            -9999.000000             -9999.000000  \n",
       "38            -9999.000000            -9999.000000             -9999.000000  \n",
       "39            -9999.000000            -9999.000000             -9999.000000  \n",
       "40            -9999.000000            -9999.000000             -9999.000000  \n",
       "41            -9999.000000            -9999.000000             -9999.000000  \n",
       "42            -9999.000000            -9999.000000             -9999.000000  \n",
       "43            -9999.000000            -9999.000000             -9999.000000  \n",
       "44            -9999.000000            -9999.000000             -9999.000000  \n",
       "45            -9999.000000            -9999.000000             -9999.000000  \n",
       "46            -9999.000000            -9999.000000             -9999.000000  \n",
       "47            -9999.000000            -9999.000000             -9999.000000  \n",
       "48            -9999.000000            -9999.000000             -9999.000000  \n",
       "49            -9999.000000            -9999.000000             -9999.000000  \n",
       "\n",
       "[50 rows x 33 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequences shape: (4620, 25, 30)\n",
      "Targets shape: (4620,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def create_lstm_input_tensor(df, n_lags=3):\n",
    "    \"\"\"\n",
    "    Create an LSTM-friendly 3D tensor from the DataFrame.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): The original DataFrame sorted by 'id' and 'date'.\n",
    "        n_lags (int): Number of days (lags) to look back for creating sequences.\n",
    "        \n",
    "    Returns:\n",
    "        numpy.ndarray: A 3D tensor for LSTM input of shape [N, D, F], \n",
    "                       where N is the number of sequences, D is the number of lags (days), \n",
    "                       and F is the number of features.\n",
    "        numpy.ndarray: An array of target values associated with each sequence.\n",
    "    \"\"\"\n",
    "    # Drop rows where 'valence' is NaN initially to determine our sequences\n",
    "    if 'valence' in df.columns:\n",
    "        df_nonan = df.dropna(subset=['valence'])\n",
    "    else:\n",
    "        df_nonan = df\n",
    "    \n",
    "    # Identify the columns to be included in the feature set (exclude 'id', 'date', and 'valence')\n",
    "    feature_columns = [col for col in df.columns if col not in ('id', 'date', 'valence')]\n",
    "    \n",
    "    sequences = []  # To store the sequences\n",
    "    targets = []    # To store the targets\n",
    "    \n",
    "    # Iterate through each unique 'id' to create sequences\n",
    "    for id_value in df_nonan['id'].unique():\n",
    "        user_df = df_nonan[df_nonan['id'] == id_value]  # Filter data for the current 'id'\n",
    "        for i in range(n_lags, len(user_df)):\n",
    "            # Create a sequence for the current index\n",
    "            seq = user_df.iloc[i-n_lags:i][feature_columns].to_numpy()\n",
    "            sequences.append(seq)\n",
    "            # Add the corresponding target value\n",
    "            targets.append(user_df.iloc[i]['valence'])\n",
    "    \n",
    "    # Convert lists to numpy arrays\n",
    "    sequences_array = np.array(sequences)\n",
    "    targets_array = np.array(targets)\n",
    "    \n",
    "    return sequences_array, targets_array\n",
    "\n",
    "# Usage example\n",
    "n_lags = 25  # Number of days to look back for lagged features\n",
    "sequences, targets = create_lstm_input_tensor(df, n_lags)\n",
    "\n",
    "print(f\"Sequences shape: {sequences.shape}\")  # Should show (N, D, F)\n",
    "print(f\"Targets shape: {targets.shape}\")      # Should show (N,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training sequences shape: (3234, 25, 30)\n",
      "Validation sequences shape: (693, 25, 30)\n",
      "Testing sequences shape: (693, 25, 30)\n",
      "Training targets shape: (3234, 3)\n",
      "Validation targets shape: (693, 3)\n",
      "Testing targets shape: (693, 3)\n"
     ]
    }
   ],
   "source": [
    "def train_val_test_split_sequences(sequences, targets, train_size=0.6, val_size=0.2):\n",
    "    \"\"\"\n",
    "    Split sequences and targets into training, validation, and testing sets while preserving chronological order.\n",
    "    \n",
    "    Args:\n",
    "        sequences (numpy.ndarray): The input sequences for the LSTM model.\n",
    "        targets (numpy.ndarray): The target values associated with each sequence.\n",
    "        train_size (float): The proportion of the dataset to include in the train split.\n",
    "        val_size (float): The proportion of the dataset to include in the validation split.\n",
    "    \n",
    "    Returns:\n",
    "        X_train, X_val, X_test, y_train, y_val, y_test: Arrays containing the splits for inputs and targets.\n",
    "    \"\"\"\n",
    "    # Calculate the split indices\n",
    "    n_sequences = len(sequences)\n",
    "    train_split_index = int(n_sequences * train_size)\n",
    "    val_split_index = train_split_index + int(n_sequences * val_size)\n",
    "    \n",
    "    # Split the data\n",
    "    X_train = sequences[:train_split_index]\n",
    "    y_train = targets[:train_split_index]\n",
    "    \n",
    "    X_val = sequences[train_split_index:val_split_index]\n",
    "    y_val = targets[train_split_index:val_split_index]\n",
    "    \n",
    "    X_test = sequences[val_split_index:]\n",
    "    y_test = targets[val_split_index:]\n",
    "    \n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test\n",
    "\n",
    "# Perform the train-validation-test split\n",
    "train_size = 0.7  # 60% of data for training\n",
    "val_size = 0.15    # 20% of data for validation\n",
    "X_train, X_val, X_test, y_train, y_val, y_test = train_val_test_split_sequences(sequences, targets, train_size, val_size)\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "y_train = to_categorical(y_train, num_classes=3)\n",
    "y_val = to_categorical(y_val, num_classes=3)\n",
    "y_test = to_categorical(y_test, num_classes=3)\n",
    "\n",
    "print(f\"Training sequences shape: {X_train.shape}\")\n",
    "print(f\"Validation sequences shape: {X_val.shape}\")\n",
    "print(f\"Testing sequences shape: {X_test.shape}\")\n",
    "print(f\"Training targets shape: {y_train.shape}\")\n",
    "print(f\"Validation targets shape: {y_val.shape}\")\n",
    "print(f\"Testing targets shape: {y_test.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.models import Sequential\n",
    "# from keras.layers import Masking, LSTM, Dense, Bidirectional\n",
    "# from keras.optimizers import Adam\n",
    "# from keras.callbacks import EarlyStopping\n",
    "\n",
    "# # Assuming the input data has been prepared\n",
    "# N, D, F = X_train.shape  # N: number of samples, D: timesteps, F: features\n",
    "\n",
    "# mask_value = -9999.0\n",
    "\n",
    "# model = Sequential()\n",
    "# model.add(Masking(mask_value=mask_value, input_shape=(D, F)))\n",
    "# #model.add(LSTM(64))  # You can adjust the number of LSTM units\n",
    "# model.add(Bidirectional(LSTM(64)))\n",
    "# model.add(Dense(32, activation='relu'))\n",
    "# model.add(Dense(3, activation='softmax'))  # Output layer for 3-class classification\n",
    "\n",
    "# # Compile the model for a multi-class classification problem\n",
    "# optimizer = Adam(learning_rate=0.0001)\n",
    "# model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# # Setup early stopping\n",
    "# early_stopping = EarlyStopping(monitor='val_loss', patience=5, verbose=1, mode='min', restore_best_weights=True)\n",
    "\n",
    "# # Assuming y_train and y_val are one-hot encoded to match the 3 classes\n",
    "# model.fit(X_train, y_train, epochs=30, batch_size=16, validation_data=(X_val, y_val), callbacks=[early_stopping])\n",
    "\n",
    "# # Ensure y_test is one-hot encoded if your model is configured for categorical outputs\n",
    "# # This step is crucial for consistency with your model's output configuration\n",
    "\n",
    "# # Evaluate the model on the test set\n",
    "# test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=1)\n",
    "\n",
    "# # Print out the model's performance on the test set\n",
    "# print(f\"Test Loss: {test_loss:.4f}\")\n",
    "# print(f\"Test Accuracy: {test_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
      "Epoch 1/300\n",
      "26/26 [==============================] - 3s 44ms/step - loss: 1.1564 - accuracy: 0.3973 - val_loss: 0.9551 - val_accuracy: 0.5945 - lr: 0.0010\n",
      "\n",
      "Epoch 2: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
      "Epoch 2/300\n",
      "26/26 [==============================] - 1s 25ms/step - loss: 0.9959 - accuracy: 0.5337 - val_loss: 0.9328 - val_accuracy: 0.5902 - lr: 0.0010\n",
      "\n",
      "Epoch 3: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
      "Epoch 3/300\n",
      "26/26 [==============================] - 1s 26ms/step - loss: 0.9726 - accuracy: 0.5489 - val_loss: 0.9800 - val_accuracy: 0.5902 - lr: 0.0010\n",
      "\n",
      "Epoch 4: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
      "Epoch 4/300\n",
      "26/26 [==============================] - 1s 27ms/step - loss: 0.9551 - accuracy: 0.5659 - val_loss: 1.0123 - val_accuracy: 0.5339 - lr: 0.0010\n",
      "\n",
      "Epoch 5: LearningRateScheduler setting learning rate to 0.0009000000427477062.\n",
      "Epoch 5/300\n",
      "26/26 [==============================] - 1s 30ms/step - loss: 0.9335 - accuracy: 0.5739 - val_loss: 1.0108 - val_accuracy: 0.5354 - lr: 9.0000e-04\n",
      "\n",
      "Epoch 6: LearningRateScheduler setting learning rate to 0.0009000000427477062.\n",
      "Epoch 6/300\n",
      "26/26 [==============================] - 1s 28ms/step - loss: 0.9085 - accuracy: 0.5897 - val_loss: 0.9886 - val_accuracy: 0.5440 - lr: 9.0000e-04\n",
      "\n",
      "Epoch 7: LearningRateScheduler setting learning rate to 0.0009000000427477062.\n",
      "Epoch 7/300\n",
      "26/26 [==============================] - 1s 27ms/step - loss: 0.9000 - accuracy: 0.5813 - val_loss: 0.9939 - val_accuracy: 0.5469 - lr: 9.0000e-04\n",
      "\n",
      "Epoch 8: LearningRateScheduler setting learning rate to 0.0009000000427477062.\n",
      "Epoch 8/300\n",
      "26/26 [==============================] - 1s 28ms/step - loss: 0.8962 - accuracy: 0.5909 - val_loss: 1.0321 - val_accuracy: 0.5296 - lr: 9.0000e-04\n",
      "\n",
      "Epoch 9: LearningRateScheduler setting learning rate to 0.0009000000427477062.\n",
      "Epoch 9/300\n",
      "26/26 [==============================] - 1s 27ms/step - loss: 0.9045 - accuracy: 0.5850 - val_loss: 1.0403 - val_accuracy: 0.5310 - lr: 9.0000e-04\n",
      "\n",
      "Epoch 10: LearningRateScheduler setting learning rate to 0.0008100000384729356.\n",
      "Epoch 10/300\n",
      "26/26 [==============================] - 1s 27ms/step - loss: 0.9042 - accuracy: 0.5863 - val_loss: 1.0300 - val_accuracy: 0.5397 - lr: 8.1000e-04\n",
      "\n",
      "Epoch 11: LearningRateScheduler setting learning rate to 0.0008100000559352338.\n",
      "Epoch 11/300\n",
      "26/26 [==============================] - 1s 28ms/step - loss: 0.8991 - accuracy: 0.5866 - val_loss: 1.0207 - val_accuracy: 0.5426 - lr: 8.1000e-04\n",
      "\n",
      "Epoch 12: LearningRateScheduler setting learning rate to 0.0008100000559352338.\n",
      "Epoch 12/300\n",
      "24/26 [==========================>...] - ETA: 0s - loss: 0.8769 - accuracy: 0.6110Restoring model weights from the end of the best epoch: 2.\n",
      "26/26 [==============================] - 1s 26ms/step - loss: 0.8789 - accuracy: 0.6098 - val_loss: 1.0040 - val_accuracy: 0.5469 - lr: 8.1000e-04\n",
      "Epoch 12: early stopping\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.9872 - accuracy: 0.6176\n",
      "Test Loss: 0.9872\n",
      "Test Accuracy: 0.6176\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Input, Bidirectional, LSTM, Dense, Dropout, BatchNormalization, GlobalAveragePooling1D, Masking\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, LearningRateScheduler\n",
    "\n",
    "\n",
    "# Define a learning rate schedule function\n",
    "def lr_schedule(epoch, lr):\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        return lr * 0.9\n",
    "    else:\n",
    "        return lr\n",
    "\n",
    "\n",
    "# Assuming the input data dimensions (N, D, F) are defined\n",
    "N, D, F = X_train.shape  # N: number of samples, D: timesteps, F: features\n",
    "mask_value = -9999.0  # The value used for padding and that should be ignored by the model\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# Adding the Masking layer\n",
    "model.add(Masking(mask_value=mask_value, input_shape=(D, F)))\n",
    "\n",
    "# Input layer is inferred in the first layer's input_shape\n",
    "#model.add(Bidirectional(LSTM(100, return_sequences=True), input_shape=(D, F)))\n",
    "model.add(LSTM(100, return_sequences=True))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "# Adding a GlobalAveragePooling1D layer to reduce the sequence to a vector\n",
    "model.add(GlobalAveragePooling1D())\n",
    "\n",
    "model.add(Dense(50, activation='relu'))\n",
    "model.add(Dropout(0.5))  # Additional dropout layer after the dense layer\n",
    "model.add(Dense(3, activation='softmax'))  # Output layer for 3-class classification\n",
    "\n",
    "# Compile the model\n",
    "lr_scheduler = LearningRateScheduler(lr_schedule, verbose=1)\n",
    "\n",
    "optimizer = Adam(learning_rate=1e-3)\n",
    "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Setup early stopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, verbose=1, mode='min', restore_best_weights=True)\n",
    "callbacks_list = [early_stopping, lr_scheduler]\n",
    "\n",
    "# Fit the model\n",
    "model.fit(X_train, y_train, epochs=300, batch_size=128, validation_data=(X_val, y_val), callbacks=callbacks_list)\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=1)\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
      "Epoch 1/50\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 632.8940 - accuracy: 0.3582 - val_loss: 5.3423 - val_accuracy: 0.3652 - lr: 0.0010\n",
      "\n",
      "Epoch 2: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
      "Epoch 2/50\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 8.9362 - accuracy: 0.3511 - val_loss: 4.8648 - val_accuracy: 0.5929 - lr: 0.0010\n",
      "\n",
      "Epoch 3: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
      "Epoch 3/50\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 9.2720 - accuracy: 0.4388 - val_loss: 8.7857 - val_accuracy: 0.5937 - lr: 0.0010\n",
      "\n",
      "Epoch 4: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
      "Epoch 4/50\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 8.5777 - accuracy: 0.4357 - val_loss: 7.8569 - val_accuracy: 0.3926 - lr: 0.0010\n",
      "\n",
      "Epoch 5: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
      "Epoch 5/50\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 11.0013 - accuracy: 0.4452 - val_loss: 10.5660 - val_accuracy: 0.5937 - lr: 0.0010\n",
      "\n",
      "Epoch 6: LearningRateScheduler setting learning rate to 0.0009048374610134307.\n",
      "Epoch 6/50\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 6.9363 - accuracy: 0.4388 - val_loss: 6.9868 - val_accuracy: 0.5929 - lr: 9.0484e-04\n",
      "\n",
      "Epoch 7: LearningRateScheduler setting learning rate to 0.0009048374486155808.\n",
      "Epoch 7/50\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 6.8543 - accuracy: 0.4381 - val_loss: 9.1105 - val_accuracy: 0.2518 - lr: 9.0484e-04\n",
      "\n",
      "Epoch 8: LearningRateScheduler setting learning rate to 0.0009048374486155808.\n",
      "Epoch 8/50\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 7.3698 - accuracy: 0.4423 - val_loss: 10.3040 - val_accuracy: 0.5937 - lr: 9.0484e-04\n",
      "\n",
      "Epoch 9: LearningRateScheduler setting learning rate to 0.0009048374486155808.\n",
      "Epoch 9/50\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 7.6184 - accuracy: 0.4381 - val_loss: 14.9000 - val_accuracy: 0.5937 - lr: 9.0484e-04\n",
      "\n",
      "Epoch 10: LearningRateScheduler setting learning rate to 0.0009048374486155808.\n",
      "Epoch 10/50\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 8.2315 - accuracy: 0.4361 - val_loss: 48.1784 - val_accuracy: 0.2357 - lr: 9.0484e-04\n",
      "\n",
      "Epoch 11: LearningRateScheduler setting learning rate to 0.0008187307807475673.\n",
      "Epoch 11/50\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 7.4559 - accuracy: 0.4423 - val_loss: 7.0418 - val_accuracy: 0.2985 - lr: 8.1873e-04\n",
      "\n",
      "Epoch 12: LearningRateScheduler setting learning rate to 0.0008187307976186275.\n",
      "Epoch 12/50\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 6.0296 - accuracy: 0.4447 - val_loss: 12.9750 - val_accuracy: 0.4079 - lr: 8.1873e-04\n",
      "\n",
      "Epoch 13: LearningRateScheduler setting learning rate to 0.0008187307976186275.\n",
      "Epoch 13/50\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 6.1373 - accuracy: 0.4356 - val_loss: 5.6141 - val_accuracy: 0.5937 - lr: 8.1873e-04\n",
      "\n",
      "Epoch 14: LearningRateScheduler setting learning rate to 0.0008187307976186275.\n",
      "Epoch 14/50\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 7.2388 - accuracy: 0.4466 - val_loss: 12.0648 - val_accuracy: 0.3049 - lr: 8.1873e-04\n",
      "\n",
      "Epoch 15: LearningRateScheduler setting learning rate to 0.0008187307976186275.\n",
      "Epoch 15/50\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 7.0876 - accuracy: 0.4419 - val_loss: 13.0217 - val_accuracy: 0.2639 - lr: 8.1873e-04\n",
      "\n",
      "Epoch 16: LearningRateScheduler setting learning rate to 0.0007408182609837606.\n",
      "Epoch 16/50\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 6.2875 - accuracy: 0.4431 - val_loss: 10.3957 - val_accuracy: 0.4312 - lr: 7.4082e-04\n",
      "\n",
      "Epoch 17: LearningRateScheduler setting learning rate to 0.0007408182718791068.\n",
      "Epoch 17/50\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 5.8567 - accuracy: 0.4414 - val_loss: 7.4066 - val_accuracy: 0.4706 - lr: 7.4082e-04\n",
      "\n",
      "Epoch 18: LearningRateScheduler setting learning rate to 0.0007408182718791068.\n",
      "Epoch 18/50\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 5.6506 - accuracy: 0.4480 - val_loss: 5.2534 - val_accuracy: 0.4328 - lr: 7.4082e-04\n",
      "\n",
      "Epoch 19: LearningRateScheduler setting learning rate to 0.0007408182718791068.\n",
      "Epoch 19/50\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 5.7113 - accuracy: 0.4506 - val_loss: 6.7280 - val_accuracy: 0.2985 - lr: 7.4082e-04\n",
      "\n",
      "Epoch 20: LearningRateScheduler setting learning rate to 0.0007408182718791068.\n",
      "Epoch 20/50\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 4.4353 - accuracy: 0.4507 - val_loss: 10.0233 - val_accuracy: 0.5937 - lr: 7.4082e-04\n",
      "\n",
      "Epoch 21: LearningRateScheduler setting learning rate to 0.0006703200923609524.\n",
      "Epoch 21/50\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 5.2874 - accuracy: 0.4493 - val_loss: 5.3049 - val_accuracy: 0.5551 - lr: 6.7032e-04\n",
      "\n",
      "Epoch 22: LearningRateScheduler setting learning rate to 0.0006703200633637607.\n",
      "Epoch 22/50\n",
      "333/363 [==========================>...] - ETA: 0s - loss: 5.1457 - accuracy: 0.4486Restoring model weights from the end of the best epoch: 2.\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 5.4928 - accuracy: 0.4490 - val_loss: 11.0000 - val_accuracy: 0.4264 - lr: 6.7032e-04\n",
      "Epoch 22: early stopping\n",
      "39/39 [==============================] - 0s 632us/step - loss: 6.1286 - accuracy: 0.5992\n",
      "Test Loss: 6.1286\n",
      "Test Accuracy: 0.5992\n"
     ]
    }
   ],
   "source": [
    "# import tensorflow as tf\n",
    "# from tensorflow.keras.models import Sequential\n",
    "# from tensorflow.keras.layers import Dense, Dropout, LayerNormalization, Embedding, MultiHeadAttention\n",
    "# from tensorflow.keras.optimizers import Adam\n",
    "# from tensorflow.keras.callbacks import EarlyStopping\n",
    "# from tensorflow.keras.callbacks import EarlyStopping, LearningRateScheduler\n",
    "# import numpy as np\n",
    "\n",
    "# # Define a learning rate schedule function\n",
    "# def lr_schedule(epoch, lr):\n",
    "#     \"\"\"\n",
    "#     Adjust the learning rate every 5 epochs.\n",
    "#     \"\"\"\n",
    "#     if epoch % 5 == 0 and epoch != 0:  # Adjust lr starting from epoch 5, every 5 epochs\n",
    "#         return lr * np.exp(-0.1)\n",
    "#     else:\n",
    "#         return lr\n",
    "\n",
    "# def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout=0):\n",
    "#     # Attention and Normalization\n",
    "#     x = LayerNormalization(epsilon=1e-6)(inputs)\n",
    "#     x = MultiHeadAttention(key_dim=head_size, num_heads=num_heads, dropout=dropout)(x, x)\n",
    "#     x = Dropout(dropout)(x)\n",
    "#     res = x + inputs\n",
    "\n",
    "#     # Feed Forward Part\n",
    "#     x = LayerNormalization(epsilon=1e-6)(res)\n",
    "#     x = Dense(ff_dim, activation='relu')(x)\n",
    "#     x = Dropout(dropout)(x)\n",
    "#     x = Dense(inputs.shape[-1])(x)\n",
    "#     return x + res\n",
    "\n",
    "# # Assuming your input data is prepared and named as X_train, X_val, y_train, y_val\n",
    "# # Set up the learning rate scheduler callback\n",
    "# lr_scheduler = LearningRateScheduler(lr_schedule, verbose=1)\n",
    "\n",
    "# # Model Configuration\n",
    "# head_size = 16\n",
    "# num_heads = 2\n",
    "# ff_dim = 16\n",
    "# dropout = 0.4\n",
    "\n",
    "# # Model Construction\n",
    "# inputs = tf.keras.Input(shape=(D, F))\n",
    "# x = transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout)\n",
    "# x = Dense(3, activation='relu')(x)\n",
    "# x = tf.keras.layers.GlobalAveragePooling1D()(x)\n",
    "# outputs = Dense(3, activation='softmax')(x)\n",
    "\n",
    "# model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "# initial_learning_rate = 1e-3\n",
    "# optimizer = Adam(learning_rate=initial_learning_rate)\n",
    "# model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# # Setup early stopping\n",
    "# early_stopping = EarlyStopping(monitor='val_loss', patience=20, verbose=1, mode='min', restore_best_weights=True)\n",
    "\n",
    "# callbacks_list = [early_stopping, lr_scheduler]\n",
    "\n",
    "# # Fit the model\n",
    "# model.fit(X_train, y_train, epochs=50, batch_size=16, validation_data=(X_val, y_val), callbacks=callbacks_list)\n",
    "\n",
    "# # Evaluate the model\n",
    "# test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=1)\n",
    "# print(f\"Test Loss: {test_loss:.4f}\")\n",
    "# print(f\"Test Accuracy: {test_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "def create_lstm_input_tensor_with_padding(df, n_lags=3):\n",
    "    \"\"\"\n",
    "    Create an LSTM-friendly 3D tensor from the DataFrame for prediction purposes,\n",
    "    including padding for initial sequences with fewer than n_lags days of history.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): The DataFrame sorted by 'id' and 'date', not containing 'valence'.\n",
    "        n_lags (int): Number of days (lags) to look back for creating sequences.\n",
    "        \n",
    "    Returns:\n",
    "        numpy.ndarray: A 3D tensor for LSTM input of shape [N, D, F], \n",
    "                       where N is the number of sequences, D is the number of lags (days), \n",
    "                       and F is the number of features.\n",
    "    \"\"\"\n",
    "    # Identify the columns to be included in the feature set (exclude 'id', 'date', and optionally 'valence' if it exists)\n",
    "    feature_columns = [col for col in df.columns if col not in ('id', 'id_kaggle', 'date', 'valence')]\n",
    "    \n",
    "    sequences = []  # To store the sequences\n",
    "    \n",
    "    # Iterate through each unique 'id' to create sequences\n",
    "    for id_value in df['id'].unique():\n",
    "        user_df = df[df['id'] == id_value]  # Filter data for the current 'id'\n",
    "        user_features = user_df[feature_columns]\n",
    "        \n",
    "        for i in range(len(user_df)):\n",
    "            if i < n_lags:\n",
    "                # Pad the sequence for rows with fewer than n_lags days of history\n",
    "                padding_size = n_lags - i\n",
    "                # padded_sequence = np.zeros((padding_size, len(feature_columns)))  # Padding with zeros\n",
    "                # padding with mask_value\n",
    "                padded_sequence = np.full((padding_size, len(feature_columns)), mask_value)\n",
    "                available_sequence = user_features.iloc[0:i].to_numpy()\n",
    "                seq = np.vstack((padded_sequence, available_sequence))\n",
    "            else:\n",
    "                seq = user_features.iloc[i-n_lags:i].to_numpy()\n",
    "            \n",
    "            sequences.append(seq)\n",
    "    \n",
    "    # Convert list to numpy array\n",
    "    sequences_array = np.array(sequences)\n",
    "    \n",
    "    return sequences_array\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv('test.csv')\n",
    "submission_cols = pd.read_csv('test_prediction.csv')\n",
    "test_df = impute_columns_mean(test_df)\n",
    "\n",
    "test_with_kaggle_id = pd.merge(test_df, submission_cols, on=['id', 'date'], how='inner')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52/52 [==============================] - 1s 3ms/step\n"
     ]
    }
   ],
   "source": [
    "sequences_for_prediction = create_lstm_input_tensor_with_padding(test_with_kaggle_id, n_lags)\n",
    "\n",
    "# Step 3: Use the model to predict\n",
    "predictions = model.predict(sequences_for_prediction)\n",
    "\n",
    "# If your model outputs categorical probabilities, you may want to convert these to class labels\n",
    "predicted_labels = np.argmax(predictions, axis=1)\n",
    "\n",
    "# Step 4 & 5: Map predictions to 'id_kaggle'\n",
    "results_df = pd.DataFrame({\n",
    "    'Id': test_with_kaggle_id['id_kaggle'],\n",
    "    'Category': predicted_labels\n",
    "})\n",
    "\n",
    "#df_merged['Category'] = \"'\" + df_merged['Category'] + \"'\"\n",
    "results_df['Category'] = results_df['Category'].astype(str)\n",
    "results_df['Category'] = \"'\" + results_df['Category'] + \"'\"\n",
    "\n",
    "\n",
    "# Adjust the filename as necessary\n",
    "results_df.to_csv('lstm.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
